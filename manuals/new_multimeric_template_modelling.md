# AlphaPulldown manual:

# Multimeric template modelling example

# Aims: Model activation of phosphoinositide 3-kinase by the influenza A virus NS1 protein (PDB: 3L4Q)
## 1st step: 3L4Q contains one bovine protein (Uniprot:P23726) and Influenza NS1 protein (Uniprot:P03496). You should create individual pickle using create_individual_features.py as normal. 
## If you have done the first step, you can skip to the **2nd step**.

In this example we refer to the NS1 protein as chain A and to the bovine protein as chain C in multimeric template 3L4Q.cif.

The rest of commands are the same as in step 1 in [Example 1](https://github.com/KosinskiLab/AlphaPulldown/blob/backend_revision/manuals/example_1.md)

## 2nd step: Predict structures using multimeric templates

#### **Task 1**
To predict structure we can use the usual ```run_multimer_jobs.py``` in custom mode (See [Example 2](https://github.com/KosinskiLab/AlphaPulldown/blob/main/manuals/example_2.md#2nd-step-predict-structures-run-on-gpu)) with an extra ```--multimeric_mode=True``` flag, that deactivates per-chain multimeric binary mask.
The user can also specify the depth of the MSA that is taken for modelling to increase the influence of the template on the predicted model. This can be done by using the flag ```--msa_depth```. Please note, that only the first 2 AlphaFold models are guided by the templates. To specify the model name you want to apply use the following flag: ```--model_names=model_1_multimer_v3,model_2_multimer_v3``` (for models 1 and 2).
If you do not know the exact MSA depth, there is another flag ```--gradient_msa_depth=True``` for exploring the desired MSA depth. This flag generates a set of logarithmically distributed points (denser at lower end) with the number of points equal to the number of predictions. The MSA depth (```num_msa```) starts from 16 and ends with the maximum value taken from the model config file. The ```extra_num_msa``` is always calculated as ```4*num_msa```.
The command line interface for using custom mode will then become:

```
run_multimer_jobs.py \
  --mode=custom \
  --num_cycle=3 \
  --num_predictions_per_model=<any number you want> \
  --output_path=<path to output directory> \
  --data_dir=<path to AlphaFold data directory> \
  --protein_lists=custom_mode.txt \
  --monomer_objects_dir=<path to features generated by create_individual_features_with_templates.py> \
  --multimeric_mode=True \
  --msa_depth=<any number you want> \
  --gradient_msa_depth=<True or False, overwrites msa_depth if provided> \
  --model_names=<coma separated names of the models> \
  --job_index=<corresponds to the string number from custom_mode.txt, don't provide for sequential execution>
```


### Running on a computer cluster in parallel

On a compute cluster, you may want to run all jobs in parallel as a [job array](https://slurm.schedmd.com/job_array.html). For example, on SLURM queuing system at EMBL we could use the following ```create_feature_jobs_SLURM.sh``` sbatch script:

```bash
#!/bin/bash

#A typical run takes couple of hours but may be much longer
#SBATCH --job-name=array
#SBATCH --time=5:00:00

#log files:
#SBATCH -e logs/create_individual_features_%A_%a_err.txt
#SBATCH -o logs/create_individual_features_%A_%a_out.txt

#qos sets priority
#SBATCH --qos=normal

#SBATCH -p htc-el8
#Limit the run to a single node
#SBATCH -N 1

#Adjust this depending on the node
#SBATCH --ntasks=8
#SBATCH --mem=32000

module load HMMER/3.3.2-gompic-2020b
module load HH-suite/3.3.0-gompic-2020b
module load Anaconda3
source activate AlphaPulldown

  create_individual_features_with_templates.py \
    --description_file=description.csv \
    --fasta_paths=fastas/P03496.fasta,fastas/P23726.fasta \
    --path_to_mmt=templates/ \
    --data_dir=/scratch/AlphaFold_DBs/2.3.2/ \
    --save_msa_files=True \
    --output_dir=features \ 
    --use_precomputed_msas=True \
    --max_template_date=2050-01-01 \
    --skip_existing=True \
    --job_index=$SLURM_ARRAY_TASK_ID
```

and the following ```run_multimer_jobs_SLURM.sh``` sbatch script:

```bash
#!/bin/bash

#A typical run takes couple of hours but may be much longer
#SBATCH --job-name=array
#SBATCH --time=2-00:00:00

#log files:
#SBATCH -e logs/run_multimer_jobs_%A_%a_err.txt
#SBATCH -o logs/run_multimer_jobs_%A_%a_out.txt

#qos sets priority
#SBATCH --qos=normal

#SBATCH -p gpu-el8

#Reserve the entire GPU so no-one else slows you down
#SBATCH --gres=gpu:1

#Limit the run to a single node
#SBATCH -N 1

#Adjust this depending on the node
#SBATCH --ntasks=8
#SBATCH --mem=64000

module load Anaconda3 
module load CUDA/11.8.0
module load cuDNN/8.7.0.84-CUDA-11.8.0
source activate AlphaPulldown

MAXRAM=$(echo `ulimit -m` '/ 1024.0'|bc)
GPUMEM=`nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits|tail -1`
export XLA_PYTHON_CLIENT_MEM_FRACTION=`echo "scale=3;$MAXRAM / $GPUMEM"|bc`
export TF_FORCE_UNIFIED_MEMORY='1'

run_multimer_jobs.py  \
  --mode=custom \
  --num_cycle=3 \
  --num_predictions_per_model=5 \
  --output_path=<path to output directory> \ 
  --data_dir=<path to AlphaFold data directory> \ 
  --protein_lists=custom_mode.txt \
  --monomer_objects_dir=/path/to/monomer_objects_directory \
  --multimeric_mode=True \
  --msa_depth=128 \
  --model_names=model_1_multimer_v3,model_2_multimer_v3 \
  --gradient_msa_depth=False \
  --job_index=$SLURM_ARRAY_TASK_ID    
```

and then run using:

```
mkdir -p logs
count=`grep -c "" description.csv` #count lines even if the last one has no end of line
sbatch --array=1-$count create_feature_jobs_SLURM.sh
count=`grep -c "" custom_mode.txt` #likewise for predictions
sbatch --array=1-$count run_multimer_jobs_SLURM.sh
```
:exclamation: To speed up computations, by default AlphaPulldown does not run relaxation (energy minimization) of models, which may decrease the quality of local geometry. If you want to enable it either only for the best models or for all predicted models, please add one of these flags to your command:
```
--models_to_relax=best
```
or
```
--models_to_relax=all
```

After the successful run one can evaluate and visualise the results in a usual manner (see e.g. [Example 2](https://github.com/KosinskiLab/AlphaPulldown/blob/main/manuals/example_2.md#2nd-step-predict-structures-run-on-gpu))
